"""Full numerical parity tests against JAX AF3 reference outputs.

These tests load pre-computed JAX AF3 outputs and weights, initialize MLX modules
with IDENTICAL weights, run forward passes, and compare outputs numerically.

This validates TRUE module-level parity against the JAX AlphaFold 3 architecture.

Requirements:
- MLX module outputs match JAX AF3 references within spec tolerances
- End-to-end RMSD < 0.5Å against JAX AF3 coordinates

Reference files location: tests/fixtures/jax_af3_refs/
Generated by: scripts/generate_jax_af3_references.py
"""

from __future__ import annotations

import json
import re
import pytest
import numpy as np
from pathlib import Path
from typing import Any

import mlx.core as mx
import mlx.nn as nn


# ============================================================================
# Reference file paths and tolerance specifications
# ============================================================================

JAX_AF3_REF_DIR = Path("tests/fixtures/jax_af3_refs")

# Tolerances from spec.md - NO RELAXATIONS
TOLERANCES = {
    "pairformer": {"rtol": 1e-4, "atol": 1e-5},
    "evoformer": {"rtol": 1e-4, "atol": 1e-5},
    "diffusion_step": {"rtol": 1e-4, "atol": 1e-5},
    "confidence": {"relative_error": 0.01},
}

# RMSD tolerance
SC003_RMSD_TOLERANCE = 0.5  # Angstrom


def _check_ref_file(path: Path, ref_name: str) -> None:
    """Fail with clear error if reference file missing."""
    if not path.exists():
        raise FileNotFoundError(
            f" reference file missing: {path}\n"
            f"Generate it with: python scripts/generate_jax_af3_references.py "
            f"--output {JAX_AF3_REF_DIR}/"
        )


def _load_manifest() -> dict[str, Any]:
    """Load the AF3_MANIFEST.json for tolerance and config info."""
    manifest_path = JAX_AF3_REF_DIR / "AF3_MANIFEST.json"
    if manifest_path.exists():
        with open(manifest_path) as f:
            return json.load(f)
    return {}


def _check_sc002_compliance(ref_data: np.lib.npyio.NpzFile) -> None:
    """Verify reference data is compliant."""
    if "sc002_compliant" in ref_data.keys():
        assert bool(ref_data["sc002_compliant"]), "Reference is NOT compliant"
    if "uses_actual_af3_module" in ref_data.keys():
        assert bool(ref_data["uses_actual_af3_module"]), (
            "Reference does NOT use actual AF3 architecture"
        )


def _check_params_exist(ref_data: np.lib.npyio.NpzFile) -> list[str]:
    """Check that params/* keys exist in reference file."""
    param_keys = [k for k in ref_data.keys() if k.startswith("params/")]
    assert len(param_keys) > 0, "No params/* keys found in reference file - NOT compliant"
    return param_keys


def _expand_colon_keys(ref_data: np.lib.npyio.NpzFile) -> dict[str, Any]:
    """Return a dict with both colon and slash variants for gather-info keys."""
    data: dict[str, Any] = {}
    for key in ref_data.keys():
        data[key] = ref_data[key]
        if ":" in key:
            data[key.replace(":", "/")] = ref_data[key]
    return data


def _find_module_prefix(ref_data: np.lib.npyio.NpzFile, module_name: str) -> str:
    """Find the params prefix for a given module name."""
    for key in ref_data.keys():
        if key.endswith(f"{module_name}/weights") or key.endswith(f"{module_name}/w"):
            return key.rsplit("/", 1)[0]
        if key.endswith(f"{module_name}/scale") or key.endswith(f"{module_name}/offset"):
            return key.rsplit("/", 1)[0]
    raise KeyError(f"Could not find params prefix for module '{module_name}'")


def _extract_layer_index(prefix: str) -> int:
    """Extract numeric layer index from a Haiku layer_stack prefix."""
    matches = re.findall(r"/(\\d+)(?:/|$)", prefix)
    if matches:
        return int(matches[-1])
    return 0


def _find_pairformer_layer_prefixes(ref_data: np.lib.npyio.NpzFile) -> list[str]:
    """Find per-layer prefixes for trunk_pairformer within Evoformer params."""
    prefixes = set()
    for key in ref_data.keys():
        if "trunk_pairformer" in key and "triangle_multiplication_outgoing" in key:
            prefix = key.split("/triangle_multiplication_outgoing/")[0]
            prefixes.add(prefix)
    return sorted(prefixes, key=_extract_layer_index)


def _iter_layer_stack_params(
    ref_data: np.lib.npyio.NpzFile, prefix: str, num_layers: int
) -> dict[int, dict[str, mx.array]]:
    """Collect stacked layer params under prefix, slicing first dim per layer."""
    layers: dict[int, dict[str, mx.array]] = {i: {} for i in range(num_layers)}
    for key in ref_data.keys():
        if not key.startswith(prefix):
            continue
        rel_path = key[len(prefix):].lstrip("/")
        values = ref_data[key]
        if values.shape[0] != num_layers:
            raise ValueError(
                f"Expected stacked params with leading dim {num_layers} for {key}, "
                f"got shape {values.shape}"
            )
        for layer_idx in range(num_layers):
            layers[layer_idx][rel_path] = mx.array(values[layer_idx])
    return layers


def _select_subparams(
    params: dict[str, mx.array], prefix: str
) -> dict[str, mx.array]:
    """Select params with a given prefix, stripping it."""
    out: dict[str, mx.array] = {}
    for key, value in params.items():
        if key.startswith(prefix):
            out[key[len(prefix):]] = value
    return out


def _find_module_root(ref_data: np.lib.npyio.NpzFile, module_name: str) -> str:
    """Find root prefix for a module inside a larger Haiku scope."""
    token = f"/{module_name}/"
    for key in ref_data.keys():
        if key.startswith("params/") and token in key:
            base = key.split(token)[0]
            return f"{base}/{module_name}"
    raise KeyError(f"Could not find root prefix for module '{module_name}'")


# ============================================================================
# Weight loading utilities for JAX → MLX conversion
# ============================================================================

def load_jax_params_to_dict(
    ref_data: np.lib.npyio.NpzFile,
    prefix: str,
) -> dict[str, mx.array]:
    """Extract JAX params from reference NPZ file with given prefix.

    Args:
        ref_data: Loaded NPZ file
        prefix: Param path prefix (e.g., 'params/pairformer/')

    Returns:
        Dict of param names (without prefix) to mx.array values
    """
    params = {}
    for key in ref_data.keys():
        if key.startswith(prefix):
            param_name = key[len(prefix):]
            params[param_name] = mx.array(ref_data[key])
    return params


def load_triangle_mult_weights(module, params: dict[str, mx.array]) -> None:
    """Load triangle multiplication weights from JAX params.

    AF3 uses 'weights' instead of 'w' for weight matrices.
    """
    # Input norm
    if "left_norm_input/scale" in params:
        module.input_norm.scale = params["left_norm_input/scale"]
    if "left_norm_input/offset" in params:
        module.input_norm.offset = params["left_norm_input/offset"]

    # Combined projection - AF3 uses 'weights' not 'w'
    if "projection/weights" in params:
        module.projection.weight = params["projection/weights"]
    elif "projection/w" in params:
        module.projection.weight = params["projection/w"]

    # Combined gate - AF3 uses 'weights' not 'w'
    if "gate/weights" in params:
        module.gate.weight = params["gate/weights"]
    elif "gate/w" in params:
        module.gate.weight = params["gate/w"]

    # Center norm
    if "center_norm/scale" in params:
        module.center_norm_scale = params["center_norm/scale"]
    if "center_norm/offset" in params:
        module.center_norm_offset = params["center_norm/offset"]

    # Output projection and gating - AF3 uses 'weights' not 'w'
    if "output_projection/weights" in params:
        module.output_projection.weight = params["output_projection/weights"]
    elif "output_projection/w" in params:
        module.output_projection.weight = params["output_projection/w"]
    if "gating_linear/weights" in params:
        module.gating_linear.weight = params["gating_linear/weights"]
    elif "gating_linear/w" in params:
        module.gating_linear.weight = params["gating_linear/w"]


def _convert_af3_qk_weights(w: mx.array) -> mx.array:
    """Convert AF3 Q/K weights (num_heads, head_dim, input_dim) to MLX (input_dim, num_heads*head_dim)."""
    if w.ndim == 3:
        num_heads, head_dim, input_dim = w.shape
        # Transpose: (h, d, i) -> (i, h, d) -> reshape to (i, h*d)
        return mx.transpose(w, (2, 0, 1)).reshape(input_dim, num_heads * head_dim)
    return w


def _convert_af3_v_weights(w: mx.array) -> mx.array:
    """Convert AF3 V weights (input_dim, num_heads, head_dim) to MLX (input_dim, num_heads*head_dim)."""
    if w.ndim == 3:
        input_dim, num_heads, head_dim = w.shape
        # Reshape (i, h, d) -> (i, h*d)
        return w.reshape(input_dim, num_heads * head_dim)
    return w


def load_grid_attention_weights(module, params: dict[str, mx.array]) -> None:
    """Load GridSelfAttention weights from JAX params.

    AF3 uses 'weights' instead of 'w' for weight matrices.
    AF3 shapes: q/k: (num_heads, head_dim, input_dim), v: (input_dim, num_heads, head_dim)
    """
    if "act_norm/scale" in params:
        module.act_norm.scale = params["act_norm/scale"]
    if "act_norm/offset" in params:
        module.act_norm.offset = params["act_norm/offset"]

    # Pair bias projection
    if "pair_bias_projection/weights" in params:
        module.pair_bias_proj.weight = params["pair_bias_projection/weights"]
    elif "pair_bias_projection/w" in params:
        module.pair_bias_proj.weight = params["pair_bias_projection/w"]

    # Q/K/V projections (AF3 shapes already match MLX)
    if "q_projection/weights" in params:
        module.q_proj.weight = params["q_projection/weights"]
    elif "q_projection/w" in params:
        module.q_proj.weight = params["q_projection/w"]
    if "k_projection/weights" in params:
        module.k_proj.weight = params["k_projection/weights"]
    elif "k_projection/w" in params:
        module.k_proj.weight = params["k_projection/w"]
    if "v_projection/weights" in params:
        module.v_proj.weight = params["v_projection/weights"]
    elif "v_projection/w" in params:
        module.v_proj.weight = params["v_projection/w"]

    # Output projection
    if "output_projection/weights" in params:
        module.o_proj.weight = params["output_projection/weights"]
    elif "output_projection/w" in params:
        module.o_proj.weight = params["output_projection/w"]

    # Gating
    if "gating_query/weights" in params:
        module.gate_proj.weight = params["gating_query/weights"]
    elif "gating_query/w" in params:
        module.gate_proj.weight = params["gating_query/w"]
    if "gating_query/bias" in params:
        module.gate_proj.bias = params["gating_query/bias"]
    elif "gating_query/b" in params:
        module.gate_proj.bias = params["gating_query/b"]


def load_single_attention_weights(module, params: dict[str, mx.array]) -> None:
    """Load single attention weights from JAX params.

    AF3 uses 'weights' instead of 'w' for weight matrices.
    """
    # Layer norm
    if "layer_norm/scale" in params:
        module.act_norm.scale = params["layer_norm/scale"]
    if "layer_norm/offset" in params:
        module.act_norm.offset = params["layer_norm/offset"]

    # Q projection (with bias)
    if "q_projection/weights" in params:
        module.q_proj.weight = params["q_projection/weights"]
    elif "q_projection/w" in params:
        module.q_proj.weight = params["q_projection/w"]
    if "q_projection/bias" in params:
        module.q_proj.bias = params["q_projection/bias"]
    elif "q_projection/b" in params:
        module.q_proj.bias = params["q_projection/b"]

    # K/V projections
    if "k_projection/weights" in params:
        module.k_proj.weight = params["k_projection/weights"]
    elif "k_projection/w" in params:
        module.k_proj.weight = params["k_projection/w"]
    if "v_projection/weights" in params:
        module.v_proj.weight = params["v_projection/weights"]
    elif "v_projection/w" in params:
        module.v_proj.weight = params["v_projection/w"]

    # Output projection (transition2 in AF3)
    if "transition2/weights" in params:
        module.o_proj.weight = params["transition2/weights"]
    elif "transition2/w" in params:
        module.o_proj.weight = params["transition2/w"]

    # Gating
    if "gating_query/weights" in params:
        module.gate_proj.weight = params["gating_query/weights"]
    elif "gating_query/w" in params:
        module.gate_proj.weight = params["gating_query/w"]
    if "gating_query/bias" in params:
        module.gate_proj.bias = params["gating_query/bias"]
    elif "gating_query/b" in params:
        module.gate_proj.bias = params["gating_query/b"]


def load_transition_weights(module, params: dict[str, mx.array]) -> None:
    """Load transition block weights from JAX params.

    AF3 uses 'weights' instead of 'w' for weight matrices.
    """
    if "input_layer_norm/scale" in params:
        module.norm.scale = params["input_layer_norm/scale"]
    if "input_layer_norm/offset" in params:
        module.norm.offset = params["input_layer_norm/offset"]

    # Transition1 - AF3 uses 'weights' not 'w'
    if "transition1/weights" in params:
        module.glu.linear.weight = params["transition1/weights"]
    elif "transition1/w" in params:
        module.glu.linear.weight = params["transition1/w"]

    # Transition2 - AF3 uses 'weights' not 'w'
    if "transition2/weights" in params:
        module.output_proj.weight = params["transition2/weights"]
    elif "transition2/w" in params:
        module.output_proj.weight = params["transition2/w"]


# ============================================================================
# AF3 Diffusion/Confidence Weight Loading Helpers (Real Modules)
# ============================================================================

def _npz_get(ref_data: np.lib.npyio.NpzFile, key: str) -> mx.array | None:
    if key in ref_data.keys():
        return mx.array(ref_data[key])
    return None


def _load_linear(linear, ref_data: np.lib.npyio.NpzFile, prefix: str) -> None:
    """Load Linear weights/bias from NPZ prefix."""
    weight = _npz_get(ref_data, f"{prefix}/weights")
    if weight is None:
        weight = _npz_get(ref_data, f"{prefix}/w")
    if weight is not None:
        linear.weight = weight

    bias = _npz_get(ref_data, f"{prefix}/bias")
    if bias is None:
        bias = _npz_get(ref_data, f"{prefix}/b")
    if bias is not None and hasattr(linear, "bias"):
        linear.bias = bias


def _load_layer_norm(ln, ref_data: np.lib.npyio.NpzFile, prefix: str) -> None:
    """Load LayerNorm scale/offset from NPZ prefix."""
    scale = _npz_get(ref_data, f"{prefix}/scale")
    if scale is not None:
        ln.scale = scale
    offset = _npz_get(ref_data, f"{prefix}/offset")
    if offset is not None:
        ln.offset = offset


def _load_adaptive_layer_norm(adaln, ref_data: np.lib.npyio.NpzFile, base: str) -> None:
    """Load AdaptiveLayerNorm params with concatenated base name."""
    _load_layer_norm(adaln.layer_norm, ref_data, f"{base}layer_norm")
    if adaln.single_cond_layer_norm is not None:
        _load_layer_norm(adaln.single_cond_layer_norm, ref_data, f"{base}single_cond_layer_norm")
    if adaln.single_cond_scale is not None:
        _load_linear(adaln.single_cond_scale, ref_data, f"{base}single_cond_scale")
    if adaln.single_cond_bias is not None:
        _load_linear(adaln.single_cond_bias, ref_data, f"{base}single_cond_bias")


def _load_adaptive_zero_init(adaz, ref_data: np.lib.npyio.NpzFile, base: str) -> None:
    """Load AdaptiveZeroInit params with concatenated base name."""
    _load_linear(adaz.transition2, ref_data, f"{base}transition2")
    if adaz.adaptive_zero_cond is not None:
        _load_linear(adaz.adaptive_zero_cond, ref_data, f"{base}adaptive_zero_cond")


def _load_transition_block(block, ref_data: np.lib.npyio.NpzFile, base: str) -> None:
    """Load diffusion_transformer.TransitionBlock params."""
    _load_adaptive_layer_norm(block.adaptive_norm, ref_data, base)
    _load_linear(block.transition1, ref_data, f"{base}transition1")
    _load_adaptive_zero_init(block.adaptive_zero, ref_data, base)


def _load_self_attention(att, ref_data: np.lib.npyio.NpzFile, base: str) -> None:
    """Load diffusion_transformer.SelfAttention params."""
    _load_adaptive_layer_norm(att.adaptive_norm, ref_data, base)
    _load_linear(att.q_projection, ref_data, f"{base}q_projection")
    _load_linear(att.k_projection, ref_data, f"{base}k_projection")
    _load_linear(att.v_projection, ref_data, f"{base}v_projection")
    _load_linear(att.gating_query, ref_data, f"{base}gating_query")
    _load_adaptive_zero_init(att.adaptive_zero, ref_data, base)


def _load_cross_attention(att, ref_data: np.lib.npyio.NpzFile, base: str) -> None:
    """Load diffusion_transformer.CrossAttention params."""
    _load_adaptive_layer_norm(att.adaptive_norm_q, ref_data, f"{base}q")
    _load_adaptive_layer_norm(att.adaptive_norm_k, ref_data, f"{base}k")
    _load_linear(att.q_projection, ref_data, f"{base}q_projection")
    _load_linear(att.k_projection, ref_data, f"{base}k_projection")
    _load_linear(att.v_projection, ref_data, f"{base}v_projection")
    _load_linear(att.gating_query, ref_data, f"{base}gating_query")
    _load_adaptive_zero_init(att.adaptive_zero, ref_data, base)


def _assign_param(obj, param_name: str, value: mx.array) -> None:
    attr_map = {
        "weights": "weight",
        "w": "weight",
        "bias": "bias",
        "b": "bias",
        "scale": "scale",
        "offset": "offset",
    }
    attr = attr_map.get(param_name)
    if attr is None:
        return
    if hasattr(obj, attr):
        setattr(obj, attr, value)


def _iter_layer_params(
    ref_data: np.lib.npyio.NpzFile, prefix: str
) -> dict[int, dict[str, mx.array]]:
    """Collect params under prefix grouped by layer index."""
    layers: dict[int, dict[str, mx.array]] = {}
    for key in ref_data.keys():
        if not key.startswith(prefix):
            continue
        parts = key.split("/")
        layer_idx = None
        layer_pos = None
        for i, part in enumerate(parts):
            if part.startswith("layer_"):
                layer_idx = int(part.split("_")[1])
                layer_pos = i
                break
            if part == "layer_stack" and i + 1 < len(parts) and parts[i + 1].isdigit():
                layer_idx = int(parts[i + 1])
                layer_pos = i + 1
                break
        if layer_idx is None or layer_pos is None:
            continue
        if layer_pos + 2 >= len(parts):
            continue
        rel_path = "/".join(parts[layer_pos + 1 :])
        layers.setdefault(layer_idx, {})[rel_path] = mx.array(ref_data[key])
    return layers


def _load_transformer(transformer, ref_data: np.lib.npyio.NpzFile, prefix: str, name_prefix: str) -> None:
    """Load diffusion_transformer.Transformer weights from NPZ."""
    _load_layer_norm(transformer.pair_input_layer_norm, ref_data, f"{prefix}/pair_input_layer_norm")
    _load_linear(transformer.pair_logits_projection, ref_data, f"{prefix}/pair_logits_projection")

    layers = _iter_layer_params(ref_data, prefix)
    for layer_idx, params in layers.items():
        if layer_idx >= len(transformer.blocks):
            continue
        block = transformer.blocks[layer_idx]
        for rel_path, value in params.items():
            parts = rel_path.split("/")
            if len(parts) < 2:
                continue
            comp = parts[0]
            param_name = parts[1]
            comp_suffix = comp[len(name_prefix):] if comp.startswith(name_prefix) else comp

            # Self-attention params
            if comp_suffix in (
                "layer_norm",
                "single_cond_layer_norm",
                "single_cond_scale",
                "single_cond_bias",
                "q_projection",
                "k_projection",
                "v_projection",
                "gating_query",
                "transition2",
                "adaptive_zero_cond",
            ):
                target = None
                if comp_suffix == "layer_norm":
                    target = block.self_attention.adaptive_norm.layer_norm
                elif comp_suffix == "single_cond_layer_norm":
                    target = block.self_attention.adaptive_norm.single_cond_layer_norm
                elif comp_suffix == "single_cond_scale":
                    target = block.self_attention.adaptive_norm.single_cond_scale
                elif comp_suffix == "single_cond_bias":
                    target = block.self_attention.adaptive_norm.single_cond_bias
                elif comp_suffix == "q_projection":
                    target = block.self_attention.q_projection
                elif comp_suffix == "k_projection":
                    target = block.self_attention.k_projection
                elif comp_suffix == "v_projection":
                    target = block.self_attention.v_projection
                elif comp_suffix == "gating_query":
                    target = block.self_attention.gating_query
                elif comp_suffix == "transition2":
                    target = block.self_attention.adaptive_zero.transition2
                elif comp_suffix == "adaptive_zero_cond":
                    target = block.self_attention.adaptive_zero.adaptive_zero_cond
                if target is not None:
                    _assign_param(target, param_name, value)
                continue

            # Transition block params
            if comp_suffix in (
                "ffw_layer_norm",
                "ffw_single_cond_layer_norm",
                "ffw_single_cond_scale",
                "ffw_single_cond_bias",
                "ffw_transition1",
                "ffw_transition2",
                "ffw_adaptive_zero_cond",
            ):
                target = None
                if comp_suffix == "ffw_layer_norm":
                    target = block.transition.adaptive_norm.layer_norm
                elif comp_suffix == "ffw_single_cond_layer_norm":
                    target = block.transition.adaptive_norm.single_cond_layer_norm
                elif comp_suffix == "ffw_single_cond_scale":
                    target = block.transition.adaptive_norm.single_cond_scale
                elif comp_suffix == "ffw_single_cond_bias":
                    target = block.transition.adaptive_norm.single_cond_bias
                elif comp_suffix == "ffw_transition1":
                    target = block.transition.transition1
                elif comp_suffix == "ffw_transition2":
                    target = block.transition.adaptive_zero.transition2
                elif comp_suffix == "ffw_adaptive_zero_cond":
                    target = block.transition.adaptive_zero.adaptive_zero_cond
                if target is not None:
                    _assign_param(target, param_name, value)


def _load_cross_att_transformer(
    transformer, ref_data: np.lib.npyio.NpzFile, prefix: str, name_prefix: str
) -> None:
    """Load diffusion_transformer.CrossAttTransformer weights from NPZ."""
    _load_layer_norm(transformer.pair_input_layer_norm, ref_data, f"{prefix}/pair_input_layer_norm")
    _load_linear(transformer.pair_logits_projection, ref_data, f"{prefix}/pair_logits_projection")

    layers = _iter_layer_params(ref_data, prefix)
    for layer_idx, params in layers.items():
        if layer_idx >= len(transformer.blocks):
            continue
        block = transformer.blocks[layer_idx]
        for rel_path, value in params.items():
            parts = rel_path.split("/")
            if len(parts) < 2:
                continue
            comp = parts[0]
            param_name = parts[1]
            comp_suffix = comp[len(name_prefix):] if comp.startswith(name_prefix) else comp

            # Cross-attention params
            if comp_suffix in (
                "q_layer_norm",
                "q_single_cond_layer_norm",
                "q_single_cond_scale",
                "q_single_cond_bias",
                "k_layer_norm",
                "k_single_cond_layer_norm",
                "k_single_cond_scale",
                "k_single_cond_bias",
                "q_projection",
                "k_projection",
                "v_projection",
                "gating_query",
                "transition2",
                "adaptive_zero_cond",
            ):
                target = None
                if comp_suffix == "q_layer_norm":
                    target = block.cross_attention.adaptive_norm_q.layer_norm
                elif comp_suffix == "q_single_cond_layer_norm":
                    target = block.cross_attention.adaptive_norm_q.single_cond_layer_norm
                elif comp_suffix == "q_single_cond_scale":
                    target = block.cross_attention.adaptive_norm_q.single_cond_scale
                elif comp_suffix == "q_single_cond_bias":
                    target = block.cross_attention.adaptive_norm_q.single_cond_bias
                elif comp_suffix == "k_layer_norm":
                    target = block.cross_attention.adaptive_norm_k.layer_norm
                elif comp_suffix == "k_single_cond_layer_norm":
                    target = block.cross_attention.adaptive_norm_k.single_cond_layer_norm
                elif comp_suffix == "k_single_cond_scale":
                    target = block.cross_attention.adaptive_norm_k.single_cond_scale
                elif comp_suffix == "k_single_cond_bias":
                    target = block.cross_attention.adaptive_norm_k.single_cond_bias
                elif comp_suffix == "q_projection":
                    target = block.cross_attention.q_projection
                elif comp_suffix == "k_projection":
                    target = block.cross_attention.k_projection
                elif comp_suffix == "v_projection":
                    target = block.cross_attention.v_projection
                elif comp_suffix == "gating_query":
                    target = block.cross_attention.gating_query
                elif comp_suffix == "transition2":
                    target = block.cross_attention.adaptive_zero.transition2
                elif comp_suffix == "adaptive_zero_cond":
                    target = block.cross_attention.adaptive_zero.adaptive_zero_cond
                if target is not None:
                    _assign_param(target, param_name, value)
                continue

            # Transition block params
            if comp_suffix in (
                "ffw_layer_norm",
                "ffw_single_cond_layer_norm",
                "ffw_single_cond_scale",
                "ffw_single_cond_bias",
                "ffw_transition1",
                "ffw_transition2",
                "ffw_adaptive_zero_cond",
            ):
                target = None
                if comp_suffix == "ffw_layer_norm":
                    target = block.transition.adaptive_norm.layer_norm
                elif comp_suffix == "ffw_single_cond_layer_norm":
                    target = block.transition.adaptive_norm.single_cond_layer_norm
                elif comp_suffix == "ffw_single_cond_scale":
                    target = block.transition.adaptive_norm.single_cond_scale
                elif comp_suffix == "ffw_single_cond_bias":
                    target = block.transition.adaptive_norm.single_cond_bias
                elif comp_suffix == "ffw_transition1":
                    target = block.transition.transition1
                elif comp_suffix == "ffw_transition2":
                    target = block.transition.adaptive_zero.transition2
                elif comp_suffix == "ffw_adaptive_zero_cond":
                    target = block.transition.adaptive_zero.adaptive_zero_cond
                if target is not None:
                    _assign_param(target, param_name, value)


def load_diffusion_head_weights(
    module,
    ref_data: np.lib.npyio.NpzFile,
    *,
    prefix: str | None = None,
) -> None:
    """Load DiffusionHead weights from JAX reference NPZ."""
    prefix = prefix or "params/diffusion_head"

    # Conditioning projections
    _load_layer_norm(module.pair_cond_initial_norm, ref_data, f"{prefix}/pair_cond_initial_norm")
    _load_linear(module.pair_cond_initial_projection, ref_data, f"{prefix}/pair_cond_initial_projection")

    _load_layer_norm(module.single_cond_initial_norm, ref_data, f"{prefix}/single_cond_initial_norm")
    _load_linear(module.single_cond_initial_projection, ref_data, f"{prefix}/single_cond_initial_projection")

    _load_layer_norm(module.noise_embedding_initial_norm, ref_data, f"{prefix}/noise_embedding_initial_norm")
    _load_linear(module.noise_embedding_initial_projection, ref_data, f"{prefix}/noise_embedding_initial_projection")

    # Pair transitions
    _load_transition_block(module.pair_transition_0, ref_data, f"{prefix}/pair_transition_0ffw_")
    _load_transition_block(module.pair_transition_1, ref_data, f"{prefix}/pair_transition_1ffw_")

    # Single transitions
    _load_transition_block(module.single_transition_0, ref_data, f"{prefix}/single_transition_0ffw_")
    _load_transition_block(module.single_transition_1, ref_data, f"{prefix}/single_transition_1ffw_")

    # Atom cross-attention encoder params
    enc = module.atom_cross_att_encoder
    _load_linear(enc.embed_ref_pos, ref_data, f"{prefix}/diffusion_embed_ref_pos")
    _load_linear(enc.embed_ref_mask, ref_data, f"{prefix}/diffusion_embed_ref_mask")
    _load_linear(enc.embed_ref_element, ref_data, f"{prefix}/diffusion_embed_ref_element")
    _load_linear(enc.embed_ref_charge, ref_data, f"{prefix}/diffusion_embed_ref_charge")
    _load_linear(enc.embed_ref_atom_name, ref_data, f"{prefix}/diffusion_embed_ref_atom_name")
    _load_linear(enc.single_to_pair_cond_row, ref_data, f"{prefix}/diffusion_single_to_pair_cond_row")
    _load_linear(enc.single_to_pair_cond_col, ref_data, f"{prefix}/diffusion_single_to_pair_cond_col")
    _load_layer_norm(enc.lnorm_trunk_single_cond, ref_data, f"{prefix}/diffusion_lnorm_trunk_single_cond")
    _load_linear(enc.embed_trunk_single_cond, ref_data, f"{prefix}/diffusion_embed_trunk_single_cond")
    _load_layer_norm(enc.lnorm_trunk_pair_cond, ref_data, f"{prefix}/diffusion_lnorm_trunk_pair_cond")
    _load_linear(enc.embed_trunk_pair_cond, ref_data, f"{prefix}/diffusion_embed_trunk_pair_cond")
    _load_linear(enc.embed_pair_offsets, ref_data, f"{prefix}/diffusion_embed_pair_offsets")
    _load_linear(enc.embed_pair_distances, ref_data, f"{prefix}/diffusion_embed_pair_distances")
    _load_linear(enc.embed_pair_offsets_valid, ref_data, f"{prefix}/diffusion_embed_pair_offsets_valid")
    _load_linear(enc.pair_mlp_1, ref_data, f"{prefix}/diffusion_pair_mlp_1")
    _load_linear(enc.pair_mlp_2, ref_data, f"{prefix}/diffusion_pair_mlp_2")
    _load_linear(enc.pair_mlp_3, ref_data, f"{prefix}/diffusion_pair_mlp_3")
    _load_linear(enc.atom_positions_to_features, ref_data, f"{prefix}/diffusion_atom_positions_to_features")
    _load_linear(enc.project_atom_features_for_aggr, ref_data, f"{prefix}/diffusion_project_atom_features_for_aggr")

    # Atom cross-attention decoder params
    dec = module.atom_cross_att_decoder
    _load_linear(dec.project_token_features_for_broadcast, ref_data, f"{prefix}/diffusion_project_token_features_for_broadcast")
    _load_layer_norm(dec.atom_features_layer_norm, ref_data, f"{prefix}/diffusion_atom_features_layer_norm")
    _load_linear(dec.atom_features_to_position_update, ref_data, f"{prefix}/diffusion_atom_features_to_position_update")

    # Cross-attention transformers (encoder/decoder)
    enc.atom_transformer_encoder._build(module.config.per_atom_channels, module.config.per_atom_pair_channels)
    dec.atom_transformer_decoder._build(module.config.per_atom_channels, module.config.per_atom_pair_channels)
    _load_cross_att_transformer(
        enc.atom_transformer_encoder,
        ref_data,
        f"{prefix}/diffusion_atom_transformer_encoder",
        "diffusion_atom_transformer_encoder",
    )
    _load_cross_att_transformer(
        dec.atom_transformer_decoder,
        ref_data,
        f"{prefix}/diffusion_atom_transformer_decoder",
        "diffusion_atom_transformer_decoder",
    )

    # Single conditioning embedding
    _load_layer_norm(module.single_cond_embedding_norm, ref_data, f"{prefix}/single_cond_embedding_norm")
    _load_linear(module.single_cond_embedding_projection, ref_data, f"{prefix}/single_cond_embedding_projection")

    # Diffusion transformer
    module.transformer._build_blocks(
        module.config.per_token_channels,
        module.config.conditioning_seq_channel,
        module.config.conditioning_pair_channel,
    )
    _load_transformer(
        module.transformer,
        ref_data,
        f"{prefix}/transformer",
        "transformer",
    )

    # Output norm
    _load_layer_norm(module.output_norm, ref_data, f"{prefix}/output_norm")


def load_confidence_head_weights(
    module,
    ref_data: np.lib.npyio.NpzFile,
    *,
    prefix: str | None = None,
) -> None:
    """Load ConfidenceHead weights from JAX reference NPZ."""
    prefix = prefix or "params/confidence_head"

    # Feature projections
    _load_linear(module.left_target_feat_project, ref_data, f"{prefix}/left_target_feat_project")
    _load_linear(module.right_target_feat_project, ref_data, f"{prefix}/right_target_feat_project")
    _load_linear(module.distogram_feat_project, ref_data, f"{prefix}/distogram_feat_project")

    # Pairformer stack (layer_stack)
    layers = _iter_layer_params(ref_data, f"{prefix}/confidence_pairformer")
    if not layers:
        # Some Haiku scopes place layer_stack before confidence_pairformer
        layers = _iter_layer_params(ref_data, prefix)
        filtered_layers: dict[int, dict[str, mx.array]] = {}
        for layer_idx, params in layers.items():
            filtered = {
                (k.split("confidence_pairformer/")[1] if "confidence_pairformer/" in k else k): v
                for k, v in params.items()
                if "confidence_pairformer/" in k
            }
            if filtered:
                filtered_layers[layer_idx] = filtered
        layers = filtered_layers
    for layer_idx, params in layers.items():
        if layer_idx >= len(module.pairformer_layers):
            continue
        layer = module.pairformer_layers[layer_idx]

        tri_out = {k.split("triangle_multiplication_outgoing/")[1]: v
                   for k, v in params.items()
                   if k.startswith("triangle_multiplication_outgoing/")}
        load_triangle_mult_weights(layer.triangle_mult_outgoing, tri_out)

        tri_in = {k.split("triangle_multiplication_incoming/")[1]: v
                  for k, v in params.items()
                  if k.startswith("triangle_multiplication_incoming/")}
        load_triangle_mult_weights(layer.triangle_mult_incoming, tri_in)

        attn1 = {k.split("pair_attention1/")[1]: v
                 for k, v in params.items()
                 if k.startswith("pair_attention1/")}
        load_grid_attention_weights(layer.pair_attention_row, attn1)

        attn2 = {k.split("pair_attention2/")[1]: v
                 for k, v in params.items()
                 if k.startswith("pair_attention2/")}
        load_grid_attention_weights(layer.pair_attention_col, attn2)

        trans = {k.split("pair_transition/")[1]: v
                 for k, v in params.items()
                 if k.startswith("pair_transition/")}
        load_transition_weights(layer.pair_transition, trans)

        if "single_pair_logits_norm/scale" in params:
            layer.pair_logits_norm.scale = params["single_pair_logits_norm/scale"]
        if "single_pair_logits_norm/offset" in params:
            layer.pair_logits_norm.offset = params["single_pair_logits_norm/offset"]
        if "single_pair_logits_projection/weights" in params:
            layer.pair_logits_proj.weight = params["single_pair_logits_projection/weights"]
        elif "single_pair_logits_projection/w" in params:
            layer.pair_logits_proj.weight = params["single_pair_logits_projection/w"]

        single_attn = {k.split("single_attention_")[1]: v
                       for k, v in params.items()
                       if k.startswith("single_attention_")}
        load_single_attention_weights(layer.single_attention, single_attn)

        single_trans = {k.split("single_transition/")[1]: v
                        for k, v in params.items()
                        if k.startswith("single_transition/")}
        load_transition_weights(layer.single_transition, single_trans)

    # Logits and norms
    _load_layer_norm(module.logits_ln, ref_data, f"{prefix}/logits_ln")
    _load_linear(module.left_half_distance_logits, ref_data, f"{prefix}/left_half_distance_logits")
    _load_layer_norm(module.pae_logits_ln, ref_data, f"{prefix}/pae_logits_ln")
    _load_linear(module.pae_logits, ref_data, f"{prefix}/pae_logits")

    _load_layer_norm(module.plddt_logits_ln, ref_data, f"{prefix}/plddt_logits_ln")
    _load_linear(module.plddt_logits, ref_data, f"{prefix}/plddt_logits")
    _load_layer_norm(module.experimentally_resolved_ln, ref_data, f"{prefix}/experimentally_resolved_ln")
    _load_linear(module.experimentally_resolved_logits, ref_data, f"{prefix}/experimentally_resolved_logits")


# ============================================================================
# PairFormer Full Parity Tests
# ============================================================================

class TestPairFormerJAXAF3Parity:
    """Full PairFormerIteration numerical parity tests against JAX AF3."""

    @pytest.fixture
    def ref_data(self) -> np.lib.npyio.NpzFile:
        """Load PairFormer reference data."""
        ref_path = JAX_AF3_REF_DIR / "pairformer_ref.npz"
        _check_ref_file(ref_path, "pairformer_ref.npz")
        return np.load(ref_path, allow_pickle=True)

    def test_pairformer_single_output_matches_jax(self, ref_data):
        """Test PairFormer single output matches JAX AF3 reference."""
        from alphafold3_mlx.network.pairformer import PairFormerIteration

        _check_sc002_compliance(ref_data)
        _check_params_exist(ref_data)

        seq_channel = int(ref_data["seq_channel"])
        pair_channel = int(ref_data["pair_channel"])
        num_heads = int(ref_data["num_heads"])

        module = PairFormerIteration(
            seq_channel=seq_channel,
            pair_channel=pair_channel,
            num_attention_heads=num_heads,
            attention_key_dim=None,
            intermediate_factor=4,
            with_single=True,
        )

        # Zero-initialize OuterProductMean if weights not in reference
        # This makes it a no-op for backward compatibility with older references
        if "params/pairformer/outer_product_mean/output_proj/w" not in ref_data.keys():
            module.outer_product_mean.output_proj.weight = mx.zeros_like(
                module.outer_product_mean.output_proj.weight
            )

        # Load weights
        tri_out_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/triangle_multiplication_outgoing/"
        )
        load_triangle_mult_weights(module.triangle_mult_outgoing, tri_out_params)

        tri_in_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/triangle_multiplication_incoming/"
        )
        load_triangle_mult_weights(module.triangle_mult_incoming, tri_in_params)

        pair_attn1_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/pair_attention1/"
        )
        load_grid_attention_weights(module.pair_attention_row, pair_attn1_params)

        pair_attn2_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/pair_attention2/"
        )
        load_grid_attention_weights(module.pair_attention_col, pair_attn2_params)

        pair_trans_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/pair_transition/"
        )
        load_transition_weights(module.pair_transition, pair_trans_params)

        # Load single path weights
        if "params/pairformer/single_pair_logits_norm/scale" in ref_data.keys():
            module.pair_logits_norm.scale = mx.array(
                ref_data["params/pairformer/single_pair_logits_norm/scale"]
            )
        if "params/pairformer/single_pair_logits_norm/offset" in ref_data.keys():
            module.pair_logits_norm.offset = mx.array(
                ref_data["params/pairformer/single_pair_logits_norm/offset"]
            )
        if "params/pairformer/single_pair_logits_projection/weights" in ref_data.keys():
            module.pair_logits_proj.weight = mx.array(
                ref_data["params/pairformer/single_pair_logits_projection/weights"]
            )
        elif "params/pairformer/single_pair_logits_projection/w" in ref_data.keys():
            module.pair_logits_proj.weight = mx.array(
                ref_data["params/pairformer/single_pair_logits_projection/w"]
            )

        single_attn_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/single_attention_"
        )
        load_single_attention_weights(module.single_attention, single_attn_params)

        single_trans_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/single_transition/"
        )
        load_transition_weights(module.single_transition, single_trans_params)

        # Load inputs
        single_input = mx.array(ref_data["single_input"])[None, ...]
        pair_input = mx.array(ref_data["pair_input"])[None, ...]
        seq_mask = mx.array(ref_data["seq_mask"])[None, ...]
        pair_mask = mx.array(ref_data["pair_mask"])[None, ...]

        # Run MLX forward
        single_out, pair_out = module(single_input, pair_input, seq_mask, pair_mask)
        mx.eval(single_out, pair_out)

        # Compare - squeeze batch dimension from both
        jax_single_out = np.array(ref_data["single_output"])
        if jax_single_out.ndim == 3 and jax_single_out.shape[0] == 1:
            jax_single_out = jax_single_out[0]
        mlx_single_out = np.array(single_out)[0]

        np.testing.assert_allclose(
            mlx_single_out,
            jax_single_out,
            rtol=TOLERANCES["pairformer"]["rtol"],
            atol=TOLERANCES["pairformer"]["atol"],
            err_msg="FAILED: PairFormer single output differs from JAX reference",
        )

        print(f"\n=== PairFormer Single Output Parity PASSED ===")
        print(f"  Max diff: {np.max(np.abs(mlx_single_out - jax_single_out)):.2e}")

    def test_pairformer_pair_output_matches_jax(self, ref_data):
        """Test PairFormer pair output matches JAX AF3 reference."""
        from alphafold3_mlx.network.pairformer import PairFormerIteration

        _check_sc002_compliance(ref_data)
        _check_params_exist(ref_data)

        seq_channel = int(ref_data["seq_channel"])
        pair_channel = int(ref_data["pair_channel"])
        num_heads = int(ref_data["num_heads"])

        module = PairFormerIteration(
            seq_channel=seq_channel,
            pair_channel=pair_channel,
            num_attention_heads=num_heads,
            attention_key_dim=None,
            intermediate_factor=4,
            with_single=True,
        )

        # Zero-initialize OuterProductMean if weights not in reference
        # This makes it a no-op for backward compatibility with older references
        if "params/pairformer/outer_product_mean/output_proj/w" not in ref_data.keys():
            module.outer_product_mean.output_proj.weight = mx.zeros_like(
                module.outer_product_mean.output_proj.weight
            )

        # Load all weights (same as single test)
        tri_out_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/triangle_multiplication_outgoing/"
        )
        load_triangle_mult_weights(module.triangle_mult_outgoing, tri_out_params)

        tri_in_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/triangle_multiplication_incoming/"
        )
        load_triangle_mult_weights(module.triangle_mult_incoming, tri_in_params)

        pair_attn1_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/pair_attention1/"
        )
        load_grid_attention_weights(module.pair_attention_row, pair_attn1_params)

        pair_attn2_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/pair_attention2/"
        )
        load_grid_attention_weights(module.pair_attention_col, pair_attn2_params)

        pair_trans_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/pair_transition/"
        )
        load_transition_weights(module.pair_transition, pair_trans_params)

        if "params/pairformer/single_pair_logits_norm/scale" in ref_data.keys():
            module.pair_logits_norm.scale = mx.array(
                ref_data["params/pairformer/single_pair_logits_norm/scale"]
            )
        if "params/pairformer/single_pair_logits_norm/offset" in ref_data.keys():
            module.pair_logits_norm.offset = mx.array(
                ref_data["params/pairformer/single_pair_logits_norm/offset"]
            )
        if "params/pairformer/single_pair_logits_projection/weights" in ref_data.keys():
            module.pair_logits_proj.weight = mx.array(
                ref_data["params/pairformer/single_pair_logits_projection/weights"]
            )
        elif "params/pairformer/single_pair_logits_projection/w" in ref_data.keys():
            module.pair_logits_proj.weight = mx.array(
                ref_data["params/pairformer/single_pair_logits_projection/w"]
            )

        single_attn_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/single_attention_"
        )
        load_single_attention_weights(module.single_attention, single_attn_params)

        single_trans_params = load_jax_params_to_dict(
            ref_data, "params/pairformer/single_transition/"
        )
        load_transition_weights(module.single_transition, single_trans_params)

        # Load inputs
        single_input = mx.array(ref_data["single_input"])[None, ...]
        pair_input = mx.array(ref_data["pair_input"])[None, ...]
        seq_mask = mx.array(ref_data["seq_mask"])[None, ...]
        pair_mask = mx.array(ref_data["pair_mask"])[None, ...]

        # Run forward
        _, pair_out = module(single_input, pair_input, seq_mask, pair_mask)
        mx.eval(pair_out)

        # Compare - squeeze batch dimension from both
        jax_pair_out = np.array(ref_data["pair_output"])
        if jax_pair_out.ndim == 4 and jax_pair_out.shape[0] == 1:
            jax_pair_out = jax_pair_out[0]
        mlx_pair_out = np.array(pair_out)[0]

        np.testing.assert_allclose(
            mlx_pair_out,
            jax_pair_out,
            rtol=TOLERANCES["pairformer"]["rtol"],
            atol=TOLERANCES["pairformer"]["atol"],
            err_msg="FAILED: PairFormer pair output differs from JAX reference",
        )

        print(f"\n=== PairFormer Pair Output Parity PASSED ===")
        print(f"  Max diff: {np.max(np.abs(mlx_pair_out - jax_pair_out)):.2e}")

    def test_pairformer_has_params_keys(self, ref_data):
        """Test that reference file has params/* keys (requirement)."""
        param_keys = _check_params_exist(ref_data)
        print(f"\n=== PairFormer Params Check PASSED ===")
        print(f"  Found {len(param_keys)} param tensors")


# ============================================================================
# Evoformer Stack NUMERICAL Parity Tests
# ============================================================================

class TestEvoformerJAXAF3Parity:
    """Evoformer stack NUMERICAL parity tests against JAX AF3."""

    @pytest.fixture
    def ref_data(self) -> np.lib.npyio.NpzFile:
        """Load Evoformer reference data."""
        ref_path = JAX_AF3_REF_DIR / "evoformer_ref.npz"
        _check_ref_file(ref_path, "evoformer_ref.npz")
        return np.load(ref_path, allow_pickle=True)

    def test_evoformer_has_params_keys(self, ref_data):
        """Test that Evoformer reference has params/* keys (requirement)."""
        param_keys = _check_params_exist(ref_data)
        assert len(param_keys) > 0, "No params found in evoformer_ref.npz"
        print(f"\n=== Evoformer Params Check PASSED ===")
        print(f"  Found {len(param_keys)} param tensors")

    def test_evoformer_numerical_parity(self, ref_data):
        """Test Evoformer numerical parity with loaded params.

        Loads JAX params into MLX module and compares final outputs.
        """
        from alphafold3_mlx.network.pairformer import PairFormerIteration
        from alphafold3_mlx.modules import Linear, LayerNorm
        from alphafold3_mlx.network.featurization import create_relative_encoding
        from alphafold3_mlx.feat_batch import TokenFeatures

        _check_sc002_compliance(ref_data)
        _check_params_exist(ref_data)

        # Config
        seq_channel = int(ref_data["seq_channel"])
        pair_channel = int(ref_data["pair_channel"])
        num_heads = int(ref_data["num_heads"])
        num_layers = int(ref_data["num_layers"])
        max_relative_idx = int(ref_data["max_relative_idx"]) if "max_relative_idx" in ref_data.keys() else 32
        max_relative_chain = int(ref_data["max_relative_chain"]) if "max_relative_chain" in ref_data.keys() else 2

        # Inputs
        target_feat = mx.array(ref_data["target_feat"])
        prev_pair = mx.array(ref_data["prev_pair"])
        prev_single = mx.array(ref_data["prev_single"])
        seq_mask = mx.array(ref_data["seq_mask"]).astype(mx.float32)

        # Token features for relative encoding
        token_features = TokenFeatures(
            token_index=mx.array(ref_data["token_index"]),
            residue_index=mx.array(ref_data["residue_index"]),
            asym_id=mx.array(ref_data["asym_id"]),
            entity_id=mx.array(ref_data["entity_id"]),
            sym_id=mx.array(ref_data["sym_id"]),
            mask=seq_mask,
        )

        rel_feat = create_relative_encoding(
            token_features, max_relative_idx=max_relative_idx, max_relative_chain=max_relative_chain
        ).astype(mx.float32)

        target_feat_dim = int(target_feat.shape[-1])
        rel_feat_dim = int(rel_feat.shape[-1])

        # Evoformer input embeddings (JAX-compatible)
        left_single = Linear(pair_channel, input_dims=target_feat_dim, use_bias=False)
        right_single = Linear(pair_channel, input_dims=target_feat_dim, use_bias=False)
        prev_pair_ln = LayerNorm(pair_channel)
        prev_pair_proj = Linear(pair_channel, input_dims=pair_channel, use_bias=False)
        position_activations = Linear(pair_channel, input_dims=rel_feat_dim, use_bias=False)

        single_proj = Linear(seq_channel, input_dims=target_feat_dim, use_bias=False)
        prev_single_ln = LayerNorm(seq_channel)
        prev_single_proj = Linear(seq_channel, input_dims=seq_channel, use_bias=False)

        # Load embedding weights
        _load_linear(left_single, ref_data, _find_module_prefix(ref_data, "left_single"))
        _load_linear(right_single, ref_data, _find_module_prefix(ref_data, "right_single"))
        _load_layer_norm(
            prev_pair_ln, ref_data, _find_module_prefix(ref_data, "prev_embedding_layer_norm")
        )
        _load_linear(prev_pair_proj, ref_data, _find_module_prefix(ref_data, "prev_embedding"))
        _load_linear(
            position_activations, ref_data, _find_module_prefix(ref_data, "position_activations")
        )
        _load_linear(
            single_proj, ref_data, _find_module_prefix(ref_data, "single_activations")
        )
        _load_layer_norm(
            prev_single_ln,
            ref_data,
            _find_module_prefix(ref_data, "prev_single_embedding_layer_norm"),
        )
        _load_linear(
            prev_single_proj, ref_data, _find_module_prefix(ref_data, "prev_single_embedding")
        )

        # Build MLX stack of PairFormer layers (from Evoformer trunk)
        layer_prefixes = _find_pairformer_layer_prefixes(ref_data)
        stacked_params = None
        if len(layer_prefixes) == 1 and "__layer_stack_no_per_layer_" in layer_prefixes[0]:
            stacked_params = _iter_layer_stack_params(
                ref_data, layer_prefixes[0], num_layers
            )
        else:
            assert len(layer_prefixes) == num_layers, (
                f"Expected {num_layers} pairformer layers, found {len(layer_prefixes)}"
            )

        layers = []
        for i in range(num_layers):
            layer = PairFormerIteration(
                seq_channel=seq_channel,
                pair_channel=pair_channel,
                num_attention_heads=num_heads,
                attention_key_dim=None,
                intermediate_factor=4,
                with_single=True,
            )

            # Zero-initialize OuterProductMean for backward compatibility
            # with older references (makes it a no-op)
            layer.outer_product_mean.output_proj.weight = mx.zeros_like(
                layer.outer_product_mean.output_proj.weight
            )

            if stacked_params is None:
                prefix = layer_prefixes[i]
                tri_out_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/triangle_multiplication_outgoing/"
                )
                tri_in_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/triangle_multiplication_incoming/"
                )
                pair_attn1_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/pair_attention1/"
                )
                pair_attn2_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/pair_attention2/"
                )
                pair_trans_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/pair_transition/"
                )
                single_attn_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/single_attention_"
                )
                single_trans_params = load_jax_params_to_dict(
                    ref_data, f"{prefix}/single_transition/"
                )
                norm_scale_key = f"{prefix}/single_pair_logits_norm/scale"
                norm_offset_key = f"{prefix}/single_pair_logits_norm/offset"
                proj_key_w = f"{prefix}/single_pair_logits_projection/weights"
                proj_key_fallback = f"{prefix}/single_pair_logits_projection/w"
            else:
                params = stacked_params[i]
                tri_out_params = _select_subparams(
                    params, "triangle_multiplication_outgoing/"
                )
                tri_in_params = _select_subparams(
                    params, "triangle_multiplication_incoming/"
                )
                pair_attn1_params = _select_subparams(params, "pair_attention1/")
                pair_attn2_params = _select_subparams(params, "pair_attention2/")
                pair_trans_params = _select_subparams(params, "pair_transition/")
                single_attn_params = _select_subparams(params, "single_attention_")
                single_trans_params = _select_subparams(params, "single_transition/")
                norm_scale_key = "single_pair_logits_norm/scale"
                norm_offset_key = "single_pair_logits_norm/offset"
                proj_key_w = "single_pair_logits_projection/weights"
                proj_key_fallback = "single_pair_logits_projection/w"

            load_triangle_mult_weights(layer.triangle_mult_outgoing, tri_out_params)
            load_triangle_mult_weights(layer.triangle_mult_incoming, tri_in_params)

            load_grid_attention_weights(layer.pair_attention_row, pair_attn1_params)
            load_grid_attention_weights(layer.pair_attention_col, pair_attn2_params)

            load_transition_weights(layer.pair_transition, pair_trans_params)

            # Single path
            if stacked_params is None:
                if norm_scale_key in ref_data.keys():
                    layer.pair_logits_norm.scale = mx.array(ref_data[norm_scale_key])
                if norm_offset_key in ref_data.keys():
                    layer.pair_logits_norm.offset = mx.array(ref_data[norm_offset_key])
                if proj_key_w in ref_data.keys():
                    layer.pair_logits_proj.weight = mx.array(ref_data[proj_key_w])
                elif proj_key_fallback in ref_data.keys():
                    layer.pair_logits_proj.weight = mx.array(ref_data[proj_key_fallback])
            else:
                if norm_scale_key in params:
                    layer.pair_logits_norm.scale = params[norm_scale_key]
                if norm_offset_key in params:
                    layer.pair_logits_norm.offset = params[norm_offset_key]
                if proj_key_w in params:
                    layer.pair_logits_proj.weight = params[proj_key_w]
                elif proj_key_fallback in params:
                    layer.pair_logits_proj.weight = params[proj_key_fallback]

            load_single_attention_weights(layer.single_attention, single_attn_params)
            load_transition_weights(layer.single_transition, single_trans_params)

            layers.append(layer)

        # Build embeddings (no batch dim yet)
        pair_act = left_single(target_feat)[:, None, :] + right_single(target_feat)[None, :, :]
        pair_act = pair_act + prev_pair_proj(prev_pair_ln(prev_pair))
        pair_act = pair_act + position_activations(rel_feat)

        single_act = single_proj(target_feat)
        single_act = single_act + prev_single_proj(prev_single_ln(prev_single))

        # Add batch dims
        pair_mask = (seq_mask[:, None] * seq_mask[None, :]).astype(mx.float32)
        single_act = single_act[None, ...]
        pair_act = pair_act[None, ...]
        seq_mask = seq_mask[None, ...]
        pair_mask = pair_mask[None, ...]

        # Run MLX forward through all layers
        for layer in layers:
            single_act, pair_act = layer(single_act, pair_act, seq_mask, pair_mask)
        mx.eval(single_act, pair_act)

        # Compare with JAX outputs - squeeze batch dimension from both
        jax_single_out = np.array(ref_data["single_output"])
        jax_pair_out = np.array(ref_data["pair_output"])
        if jax_single_out.ndim == 3 and jax_single_out.shape[0] == 1:
            jax_single_out = jax_single_out[0]
        if jax_pair_out.ndim == 4 and jax_pair_out.shape[0] == 1:
            jax_pair_out = jax_pair_out[0]

        mlx_single_out = np.array(single_act)[0]
        mlx_pair_out = np.array(pair_act)[0]

        np.testing.assert_allclose(
            mlx_single_out,
            jax_single_out,
            rtol=TOLERANCES["evoformer"]["rtol"],
            atol=TOLERANCES["evoformer"]["atol"],
            err_msg="FAILED: Evoformer single output differs from JAX reference",
        )

        np.testing.assert_allclose(
            mlx_pair_out,
            jax_pair_out,
            rtol=TOLERANCES["evoformer"]["rtol"],
            atol=TOLERANCES["evoformer"]["atol"],
            err_msg="FAILED: Evoformer pair output differs from JAX reference",
        )

        print(f"\n=== Evoformer Numerical Parity PASSED ===")
        print(f"  Single max diff: {np.max(np.abs(mlx_single_out - jax_single_out)):.2e}")
        print(f"  Pair max diff: {np.max(np.abs(mlx_pair_out - jax_pair_out)):.2e}")


# ============================================================================
# Diffusion Step NUMERICAL Parity Tests
# ============================================================================

class TestDiffusionStepJAXAF3Parity:
    """Diffusion head NUMERICAL parity tests using real AF3 DiffusionHead."""

    @pytest.fixture
    def ref_data(self) -> np.lib.npyio.NpzFile:
        """Load diffusion step reference data."""
        ref_path = JAX_AF3_REF_DIR / "diffusion_step_ref.npz"
        _check_ref_file(ref_path, "diffusion_step_ref.npz")
        return np.load(ref_path, allow_pickle=True)

    def test_diffusion_has_params_keys(self, ref_data):
        """Test that diffusion reference has params/* keys (requirement)."""
        param_keys = _check_params_exist(ref_data)
        print(f"\n=== Diffusion Params Check PASSED ===")
        print(f"  Found {len(param_keys)} param tensors")

    def test_diffusion_head_numerical_parity(self, ref_data):
        """Test MLX DiffusionHead numerical parity vs JAX AF3 reference."""
        from alphafold3_mlx.network.diffusion_head import DiffusionHead
        from alphafold3_mlx.core.config import DiffusionConfig, GlobalConfig
        from alphafold3_mlx.feat_batch import Batch

        _check_sc002_compliance(ref_data)
        _check_params_exist(ref_data)

        # Build Batch from reference dict
        batch_keys = [
            "token_index",
            "residue_index",
            "asym_id",
            "entity_id",
            "sym_id",
            "seq_mask",
            "pred_dense_atom_mask",
            "ref_pos",
            "ref_mask",
            "ref_element",
            "ref_charge",
            "ref_atom_name_chars",
            "ref_space_uid",
            # Atom cross-att gather info
            "token_atoms_to_queries/gather_idxs",
            "token_atoms_to_queries/gather_mask",
            "token_atoms_to_queries/input_shape",
            "tokens_to_queries/gather_idxs",
            "tokens_to_queries/gather_mask",
            "tokens_to_queries/input_shape",
            "tokens_to_keys/gather_idxs",
            "tokens_to_keys/gather_mask",
            "tokens_to_keys/input_shape",
            "queries_to_keys/gather_idxs",
            "queries_to_keys/gather_mask",
            "queries_to_keys/input_shape",
            "queries_to_token_atoms/gather_idxs",
            "queries_to_token_atoms/gather_mask",
            "queries_to_token_atoms/input_shape",
            # Pseudo-beta gather info
            "token_atoms_to_pseudo_beta/gather_idxs",
            "token_atoms_to_pseudo_beta/gather_mask",
            "token_atoms_to_pseudo_beta/input_shape",
        ]
        ref_dict = _expand_colon_keys(ref_data)
        batch_dict = {k: ref_dict[k] for k in batch_keys if k in ref_dict}
        batch = Batch.from_data_dict(batch_dict)

        # Inputs
        positions_noisy = mx.array(ref_data["positions_noisy"])
        noise_level = mx.array(ref_data["noise_level"])
        embeddings = {
            "single": mx.array(ref_data["embeddings_single"]),
            "pair": mx.array(ref_data["embeddings_pair"]),
            "target_feat": mx.array(ref_data["embeddings_target_feat"]),
        }
        use_conditioning = bool(ref_data["use_conditioning"])

        # Config
        seq_channel = int(ref_data["seq_channel"])
        pair_channel = int(ref_data["pair_channel"])
        config = DiffusionConfig(
            conditioning_seq_channel=seq_channel,
            conditioning_pair_channel=pair_channel,
        )
        module = DiffusionHead(config=config, global_config=GlobalConfig())

        # Build conditioning shapes before loading weights
        module._build_conditioning(
            pair_cond_dim=embeddings["pair"].shape[-1],
            single_cond_dim=embeddings["single"].shape[-1] + embeddings["target_feat"].shape[-1],
        )

        # Load weights
        load_diffusion_head_weights(module, ref_data)

        # Forward
        denoised = module(
            positions_noisy=positions_noisy,
            noise_level=noise_level,
            batch=batch,
            embeddings=embeddings,
            use_conditioning=use_conditioning,
        )
        mx.eval(denoised)

        # Compare
        jax_out = ref_data["denoised_positions"]
        np.testing.assert_allclose(
            np.array(denoised),
            jax_out,
            rtol=TOLERANCES["diffusion_step"]["rtol"],
            atol=TOLERANCES["diffusion_step"]["atol"],
            err_msg="FAILED: DiffusionHead output differs from JAX reference",
        )

        print(f"\n=== DiffusionHead Numerical Parity PASSED ===")
        print(f"  Max diff: {np.max(np.abs(np.array(denoised) - jax_out)):.2e}")


# ============================================================================
# Confidence Head NUMERICAL Parity Tests
# ============================================================================

class TestConfidenceHeadJAXAF3Parity:
    """Confidence head NUMERICAL parity tests using real AF3 ConfidenceHead."""

    @pytest.fixture
    def ref_data(self) -> np.lib.npyio.NpzFile:
        """Load confidence head reference data."""
        ref_path = JAX_AF3_REF_DIR / "confidence_ref.npz"
        _check_ref_file(ref_path, "confidence_ref.npz")
        return np.load(ref_path, allow_pickle=True)

    def test_confidence_has_params_keys(self, ref_data):
        """Test that confidence reference has params/* keys (requirement)."""
        param_keys = _check_params_exist(ref_data)
        print(f"\n=== Confidence Params Check PASSED ===")
        print(f"  Found {len(param_keys)} param tensors")

    def test_confidence_head_numerical_parity(self, ref_data):
        """Test MLX ConfidenceHead numerical parity vs JAX AF3 reference."""
        from alphafold3_mlx.network.confidence_head import ConfidenceHead
        from alphafold3_mlx.core.config import ConfidenceConfig, GlobalConfig
        from alphafold3_mlx.atom_layout import GatherInfo

        _check_sc002_compliance(ref_data)
        _check_params_exist(ref_data)

        # Inputs
        dense_atom_positions = mx.array(ref_data["dense_atom_positions"])
        embeddings = {
            "single": mx.array(ref_data["embeddings_single"]),
            "pair": mx.array(ref_data["embeddings_pair"]),
            "target_feat": mx.array(ref_data["embeddings_target_feat"]),
        }
        seq_mask = mx.array(ref_data["seq_mask"])
        asym_id = mx.array(ref_data["asym_id"])

        # Gather info
        ref_dict = _expand_colon_keys(ref_data)
        token_atoms_to_pseudo_beta = GatherInfo.from_dict(
            ref_dict, "token_atoms_to_pseudo_beta"
        )

        # Config
        config = ConfidenceConfig(
            num_plddt_bins=int(ref_data["num_plddt_bins"]),
            num_bins=int(ref_data["num_bins"]) if "num_bins" in ref_data.keys() else int(ref_data["num_pae_bins"]),
            num_pae_bins=int(ref_data["num_pae_bins"]),
            max_error_bin=float(ref_data["max_error_bin"]),
        )
        module = ConfidenceHead(
            config=config,
            global_config=GlobalConfig(),
            seq_channel=embeddings["single"].shape[-1],
            pair_channel=embeddings["pair"].shape[-1],
        )

        # Build before weight load
        module._build(embeddings["target_feat"].shape[-1], dense_atom_positions.shape[-2])
        load_confidence_head_weights(module, ref_data)

        # Forward
        result = module(
            dense_atom_positions=dense_atom_positions,
            embeddings=embeddings,
            seq_mask=seq_mask,
            token_atoms_to_pseudo_beta=token_atoms_to_pseudo_beta,
            asym_id=asym_id,
        )
        mx.eval(result.plddt, result.pae, result.pde)

        # Compare with JAX outputs
        jax_plddt = ref_data["predicted_lddt"]
        jax_pae = ref_data["full_pae"]
        jax_pde = ref_data["full_pde"]

        def rel_error(a, b, eps=1e-8):
            return np.max(np.abs(a - b) / np.maximum(np.abs(b), eps))

        plddt_err = rel_error(np.array(result.plddt)[0], jax_plddt)
        pae_err = rel_error(np.array(result.pae)[0], jax_pae)
        pde_err = rel_error(np.array(result.pde)[0], jax_pde)

        assert plddt_err < TOLERANCES["confidence"]["relative_error"], (
            f"FAILED: pLDDT relative error {plddt_err:.4f} exceeds tolerance"
        )
        assert pae_err < TOLERANCES["confidence"]["relative_error"], (
            f"FAILED: PAE relative error {pae_err:.4f} exceeds tolerance"
        )
        assert pde_err < TOLERANCES["confidence"]["relative_error"], (
            f"FAILED: PDE relative error {pde_err:.4f} exceeds tolerance"
        )

        print(f"\n=== ConfidenceHead Numerical Parity PASSED ===")
        print(f"  pLDDT rel error: {plddt_err:.4f}")
        print(f"  PAE rel error: {pae_err:.4f}")
        print(f"  PDE rel error: {pde_err:.4f}")


# ============================================================================
# End-to-End RMSD Test
# ============================================================================

class TestSC003EndToEndRMSD:
    """End-to-end RMSD parity vs real JAX AF3 Model outputs."""

    @pytest.fixture
    def ref_data(self) -> np.lib.npyio.NpzFile:
        """Load end-to-end reference data."""
        ref_path = JAX_AF3_REF_DIR / "end_to_end_ref.npz"
        _check_ref_file(ref_path, "end_to_end_ref.npz")
        return np.load(ref_path, allow_pickle=True)

    def test_end_to_end_has_params_keys(self, ref_data):
        """Test that end-to-end reference has params/* keys (requirement)."""
        param_keys = _check_params_exist(ref_data)
        print(f"\n=== End-to-End Params Check PASSED ===")
        print(f"  Found {len(param_keys)} param tensors")

    def test_end_to_end_rmsd_parity(self, ref_data):
        """Test MLX Model RMSD vs JAX AF3 output."""
        from alphafold3_mlx.model import Model
        from alphafold3_mlx.core.config import (
            ModelConfig,
            DiffusionConfig,
            ConfidenceConfig,
            GlobalConfig,
        )
        from alphafold3_mlx.feat_batch import Batch

        _check_params_exist(ref_data)

        # Build parity Batch from reference dict
        batch_keys = [
            "token_index",
            "residue_index",
            "asym_id",
            "entity_id",
            "sym_id",
            "seq_mask",
            "pred_dense_atom_mask",
            "ref_pos",
            "ref_mask",
            "ref_element",
            "ref_charge",
            "ref_atom_name_chars",
            "ref_space_uid",
            # Atom cross-att gather info
            "token_atoms_to_queries/gather_idxs",
            "token_atoms_to_queries/gather_mask",
            "token_atoms_to_queries/input_shape",
            "tokens_to_queries/gather_idxs",
            "tokens_to_queries/gather_mask",
            "tokens_to_queries/input_shape",
            "tokens_to_keys/gather_idxs",
            "tokens_to_keys/gather_mask",
            "tokens_to_keys/input_shape",
            "queries_to_keys/gather_idxs",
            "queries_to_keys/gather_mask",
            "queries_to_keys/input_shape",
            "queries_to_token_atoms/gather_idxs",
            "queries_to_token_atoms/gather_mask",
            "queries_to_token_atoms/input_shape",
            # Pseudo-beta gather info
            "token_atoms_to_pseudo_beta/gather_idxs",
            "token_atoms_to_pseudo_beta/gather_mask",
            "token_atoms_to_pseudo_beta/input_shape",
        ]
        ref_dict = _expand_colon_keys(ref_data)
        batch_dict = {k: ref_dict[k] for k in batch_keys if k in ref_dict}
        batch = Batch.from_data_dict(batch_dict)

        # Override embeddings from JAX reference
        embeddings = {
            "single": mx.array(ref_data["single_embeddings"]),
            "pair": mx.array(ref_data["pair_embeddings"]),
            "target_feat": mx.array(ref_data["target_feat"]),
        }

        seq_channel = int(ref_data["seq_channel"])
        pair_channel = int(ref_data["pair_channel"])

        global_config = GlobalConfig(bfloat16="none", final_init="zeros")
        evoformer_cfg = ModelConfig.default().evoformer
        evoformer_cfg.seq_channel = seq_channel
        evoformer_cfg.pair_channel = pair_channel

        diff_cfg = DiffusionConfig(
            num_steps=int(ref_data["diffusion_steps"]),
            num_samples=int(ref_data["diffusion_num_samples"]),
            conditioning_seq_channel=seq_channel,
            conditioning_pair_channel=pair_channel,
        )

        conf_cfg = ConfidenceConfig()
        if "num_plddt_bins" in ref_data.keys():
            conf_cfg.num_plddt_bins = int(ref_data["num_plddt_bins"])
        if "num_pae_bins" in ref_data.keys():
            conf_cfg.num_pae_bins = int(ref_data["num_pae_bins"])
            conf_cfg.num_bins = int(ref_data["num_pae_bins"])
        if "max_error_bin" in ref_data.keys():
            conf_cfg.max_error_bin = float(ref_data["max_error_bin"])

        model_cfg = ModelConfig(
            evoformer=evoformer_cfg,
            diffusion=diff_cfg,
            confidence=conf_cfg,
            global_config=global_config,
            num_recycles=int(ref_data["num_recycles"]),
            return_embeddings=True,
        )
        model = Model(model_cfg)

        # Build modules with correct dims before weight loading
        model.diffusion_head._build_conditioning(
            pair_cond_dim=embeddings["pair"].shape[-1],
            single_cond_dim=embeddings["single"].shape[-1] + embeddings["target_feat"].shape[-1],
        )
        model.confidence_head._build(
            embeddings["target_feat"].shape[-1], int(ref_data["num_atoms"])
        )

        # Load weights from end-to-end reference
        diff_prefix = _find_module_root(ref_data, "diffusion_head")
        conf_prefix = _find_module_root(ref_data, "confidence_head")
        load_diffusion_head_weights(model.diffusion_head, ref_data, prefix=diff_prefix)
        load_confidence_head_weights(model.confidence_head, ref_data, prefix=conf_prefix)

        diffusion_override = {
            "positions_noisy_steps": ref_data["positions_noisy_steps"],
            "t_hat_steps": ref_data["t_hat_steps"],
            "noise_levels": ref_data["noise_levels"],
        }

        # Run model with overrides
        result = model(
            batch,
            key=mx.random.key(0),
            override_embeddings=embeddings,
            diffusion_override=diffusion_override,
        )
        mx.eval(result.atom_positions.positions)

        jax_positions = ref_data["atom_positions"]
        jax_mask = ref_data["atom_positions_mask"]
        mlx_positions = np.array(result.atom_positions.positions)
        mlx_mask = np.array(result.atom_positions.mask)

        valid_mask = (jax_mask > 0.5) & (mlx_mask > 0.5)
        diff = jax_positions[valid_mask] - mlx_positions[valid_mask]
        rmsd = np.sqrt(np.mean(np.sum(diff ** 2, axis=-1)))

        assert rmsd < SC003_RMSD_TOLERANCE, (
            f"FAILED: RMSD {rmsd:.4f}Å exceeds {SC003_RMSD_TOLERANCE:.2f}Å"
        )

        print(f"\n=== End-to-End RMSD Parity PASSED ===")
        print(f"  RMSD: {rmsd:.4f} Å")


# ============================================================================
# Test runner
# ============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-x"])
